import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, accuracy_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Load the dataset
data_path = 'C:\\Users\\john3\\Desktop\\For_Submission\\400dataset.csv'
data = pd.read_csv(data_path, low_memory=False, encoding='utf-8')
print(data['class'].unique())

# Drop rows with any NaN in the target or impute them
# Ensure 'class' is your target variable
if data['class'].isnull().any():
    # Option 1: Drop rows with NaN in the target variable
    data = data.dropna(subset=['class'])
    # Option 2: Impute NaNs in the target variable (if appropriate)
    # imputer = SimpleImputer(strategy='most_frequent')
    # data['class'] = imputer.fit_transform(data[['class']])

# Assuming 'filename' is a unique identifier and not a feature, and 'class' is the target variable
X = data.drop(['class', 'filename'], axis=1, errors='ignore')
y = data['class']

# Convert all non-numeric columns to numeric (if any)
# This is assuming your feature matrix should contain only numeric values
# Any non-numeric data will be converted to NaN and then imputed to 0
X = X.apply(pd.to_numeric, errors='coerce').fillna(0)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize classifiers
classifiers = {
    'Support Vector Machine': SVC(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier()
}

# Train and evaluate each classifier
# Train and evaluate each classifier
for name, clf in classifiers.items():
    # Train the classifier on the training data
    clf.fit(X_train, y_train)
    
    # Make predictions on the testing data
    y_pred = clf.predict(X_test)
    
    # Evaluate the classifier
    accuracy = accuracy_score(y_test, y_pred)
    class_report = classification_report(y_test, y_pred, zero_division=1)
    
    print(f"{name} Accuracy: {accuracy * 100:.2f}%")
    print(f"Classification Report for {name}:\n{class_report}\n")